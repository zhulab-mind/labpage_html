
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>MIND Research</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="description" content="This is the official web page for the MIND Lab at UTA.">
    <meta name="theme-color" content="#052049">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="static/css/custom.css">
    <link href="https://fonts.googleapis.com/css2?family=Lobster&family=Montserrat:wght@700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Pacifico&display=swap" rel="stylesheet">
    

    <!-- <link href="/static/css/bootstrap.css" rel="stylesheet"> -->
    <!-- <link href="/static/css/bootstrap-responsive.css" rel="stylesheet"> -->
    <link href="static/css/syntax.css" rel="stylesheet">
    <link href="static/css/custom.css" rel="stylesheet">

    <script type="text/javascript" src="https://www.google.com/jsapi"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>

  <body>

    <!-- Static navbar -->
    <nav class="navbar navbar-expand-md navbar-dark" style="background-color: #0064B1">
        <a class="navbar-brand" href="index.html">MIND Lab</a>
        <a href="https://www.uta.edu/academics/faculty/profile?username=zhud2" target="_blank"><img class="inline-block navb-icon" src="static/img/logos/uta-logo.png" alt="UTA" style="width: 55px; height: auto;"></a>
        <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbarContent" aria-controls="navbarContent" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarContent">
          <ul class="navbar-nav ml-auto">
            
              
                <li class="nav-item"><a class="nav-link" href="index.html" alt="Home"> Home</a></li>
              
            
              
                <li class = "nav-item active"><a class="nav-link" href="research.html" alt="Research"> Research<span class="sr-only">(current)</span></a></li>
              
            
              
                <li class="nav-item"><a class="nav-link" href="publicatiion.html" alt="Publications"> Publications</a></li>
              
            
              
                <li class="nav-item"><a class="nav-link" href="members.html" alt="Members"> Members</a></li>
              
            
          </ul>
        </div><!--/.nav-collapse -->
      </nav>

    <div class="container">


    <!--  Previous navbar bootstrap 3.2 -->
    <!--
      <div class="navbar navbar-fixed-top navbar-default" role="navigation">
        <div class="container-fluid">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">MIND Lab</a>
          </div>
          <div class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
              
                
                  <li><a href="/" alt="Home"> Home</a></li>
                
              
                
                  <li class = "active"><a href="/research" alt="Research"> Research</a></li>
                
              
                
                  <li><a href="/publications" alt="Publications"> Publications</a></li>
                
              
                
                  <li><a href="/members" alt="Members"> Members</a></li>
                
              
              -->
              <!-- <li><a href="https://sites.google.com/a/fraserlab.com/ucsf-private/" alt="Lab Private Area">private</a></li> -->
           <!-- </ul>
          </div>
        </div>
      </div>

    <div class="container"> -->


<div class="container px-0 max-width: 100%">
    <div class="row">
        <div class="col">
            <div class = "page header"><h1>MIND Research</h1></div>
                <style>
   h1, .post-title {
    text-align: center; /* 居中 */
    font-family: 'Playfair Display', serif; /* 使用优雅的衬线字体 */
    font-size: 3em; /* 调整字体大小 */
    font-weight: bold; /* 加粗 */
    color: #2a7ae2; /* 设定字体颜色 */
    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); /* 添加阴影 */
    letter-spacing: 2px; /* 调整字母间距 */
  }
  p, li {
    text-align: justify; /* 两端对齐 */
    width: 90%; /* 设置宽度为页面列宽比例 */
    max-width: 1000px; /* 确保过宽屏幕下不超出范围 */
    margin: 0 auto; /* 居中对齐 */
    line-height: 1.6; /* 行高 */;
  }
  img {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: auto;
    max-width: 100%; /* 图片宽度随容器调整 */
    height: auto;
  }
</style>

<h2 id="brain-imaging"><strong>Brain Imaging</strong></h2>
<h4 id="learning-lifespan-brain-anatomical-correspondence-via-cortical-developmental-continuity-transfer-medical-image-analysis-2024"><a href="https://www.sciencedirect.com/science/article/pii/S1361841524002536?casa_token=PyunmY4ukk8AAAAA:3ljJmw3chie2GBAD2iq56kV_IsrocRM-XaqdBSHaZVQOhEny114H2kk-sBwpinfdqoTscxjO">Learning Lifespan Brain Anatomical Correspondence via Cortical Developmental Continuity Transfer</a>, Medical Image Analysis, 2024</h4>

<p><img src="static/img/research/lifespan.png" class="responsive" alt="Eigenmode" /></p>

<p>Due to the variability in cortical folding, neurodevelopmental stages, and limited neuroimaging data, inferring reliable lifespan anatomical correspondences is challenging. To address this, we leverage cortical developmental continuity and propose a transfer learning strategy: training the model on the largest age group and adapting it to other groups along the cortical trajectory. Evaluated on 1,000+ brains across four age groups (34 gestational weeks to young adults), results show that this strategy significantly improves performance in populations with limited samples and robustly infers complex anatomical correspondences across stages.
<br /><br /></p>

<h4 id="cortex2vector-anatomical-embedding-of-cortical-folding-patterns-cerebral-cortex-2022"><a href="https://academic.oup.com/cercor/article/33/10/5851/6880883?login=false">Cortex2vector: anatomical embedding of cortical folding patterns</a>, Cerebral Cortex, 2022</h4>

<p><img src="static/img/research/cortex2vector.png" class="responsive" alt="Eigenmode" /></p>

<p>In this work, we put further effort, based on the identified 3HGs, to establish the correspondences of individual 3HGs. We developed a learning-based embedding framework to encode individual cortical folding patterns into a group of anatomically meaningful embedding vectors (cortex2vector). Each 3HG can be represented as a combination of these embedding vectors via a set of individual specific combining coefficients. In this way, the regularity of folding pattern is encoded into the embedding vectors, while the individual variations are preserved by the multi-hop combination coefficients.
<br /><br /></p>

<h4 id="predicting-brain-structural-network-using-functional-connectivity-medical-image-analysis-2022"><a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841522001104?casa_token=fPzR05pu_9AAAAAA:Oe6JP8ElZudqz3yqBE51fqbLa0IH8f5lyMm6bbOrqWJowXUI6DzrNUF_2MnpzlWD5dBlDy-TKyxZ">Predicting brain structural network using functional connectivity</a>, Medical Image Analysis, 2022</h4>

<p><img src="static/img/research/Predicting brain structural network using functional connectivity.png" class="responsive" alt="Eigenmode" /></p>

<p>Motivated by the advances of generative adversarial network (GAN) and graph convolutional network (GCN) in the deep learning field, in this work, we proposed a multi-GCN based GAN (MGCN-GAN) to infer individual SC based on corresponding FC by automatically learning the complex associations between individual brain structural and functional networks. The generator of MGCN-GAN is composed of multiple multi-layer GCNs which are designed to model complex indirect connections in brain network. The discriminator of MGCN-GAN is a single multi-layer GCN which aims to distinguish the predicted SC from real SC. To overcome the inherent unstable behavior of GAN, we designed a new structure-preserving (SP) loss function to guide the generator to learn the intrinsic SC patterns more effectively. Using Human Connectome Project (HCP) dataset and Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset as test beds, our MGCN-GAN model can generate reliable individual SC from FC. 
<br /><br /></p>

<h2 id="brain-disease"><strong>Brain Disease</strong></h2>
<h4 id="disease2vec-representing-alzheimers-progression-via-disease-embedding-tree-pharmacological-research-2023"><a href="https://www.sciencedirect.com/science/article/pii/S1043661823003948">Disease2Vec: Representing Alzheimer’s Progression via Disease Embedding Tree</a>, Pharmacological Research, 2023</h4>

<p><img src="static/img/research/disease2vec.png" class="responsive" alt="Eigenmode" /></p>

<p>Predictive models for Alzheimer’s Disease (AD) and mild cognitive impairment (MCI) have largely focused on binary or multi-class classification, overlooking AD’s continuous progression. While recent models explore biomarker sequences, predicting individual patient status across AD’s continuous stages remains understudied. We introduce Disease2Vec, a novel embedding framework that encodes clinical stages into meaningful vectors, creating a disease embedding tree (DETree) that reflects AD progression. DETree enables accurate prediction across five clinical groups and provides richer insights by mapping patients onto a continuous trajectory of AD progression.
<br /><br /></p>

<h4 id="deep-fusion-of-brain-structure-function-in-mild-cognitive-impairment-medical-image-analysis-2021"><a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841521001286">Deep Fusion of Brain Structure-Function in Mild Cognitive Impairment</a>, Medical image analysis, 2021</h4>

<p><img src="static/img/research/Deep Fusion of Brain Structure-Function in Mild Cognitive Impairment.png" class="responsive" alt="Eigenmode" /></p>

<p>In this work, we developed a graph-based deep neural network to simultaneously model brain structure and function in Mild Cognitive Impairment (MCI): the topology of the graph is initialized using structural network (from diffusion MRI) and iteratively updated by incorporating functional information (from functional MRI) to maximize the capability of differentiating MCI patients from elderly normal controls. This resulted in a new connectome by exploring “deep relations” between brain structure and function in MCI patients and we named it as Deep Brain Connectome. Though deep brain connectome is learned individually, it shows consistent patterns of alteration comparing to structural network at group level.
<br /><br /></p>

<h4 id="multimodal-deep-fusion-in-hyperbolic-space-for-mild-cognitive-impairment-study-miccai-2023"><a href="https://link.springer.com/chapter/10.1007/978-3-031-43904-9_65">Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study</a>, MICCAI, 2023</h4>

<p><img src="static/img/research/Hyperbolic.png" class="responsive" alt="Eigenmode" /></p>

<p>Recent studies have suggested that non-Euclidean hyperbolic space may provide a more accurate interpretation of brain connectomes than Euclidean space. In light of these findings, we propose a novel graph-based hyperbolic deep model with a learnable topology to integrate the individual structural network with functional information in hyperbolic space for the MCI/NC (normal control) classification task.
<br /><br /></p>

<h2 id="brain-inspired-ai"><strong>Brain-inspired AI</strong></h2>
<h4 id="bi-avan-a-brain-inspired-adversarial-visual-attention-network-for-characterizing-human-visual-attention-from-neural-activity-ieee-transactions-on-multimedia-2024"><a href="https://ieeexplore.ieee.org/abstract/document/10636811">BI-AVAN: A Brain-Inspired Adversarial Visual Attention Network for Characterizing Human Visual Attention from Neural Activity</a>, IEEE Transactions on Multimedia, 2024</h4>

<p><img src="static/img/research/BI-AVAN.png" class="responsive" alt="Eigenmode" /></p>

<p>Most visual attention studies rely on eye-tracking rather than directly measuring brain activity, and they overlook the adversarial relationship between attention-related objects and background. To address this, we propose a brain-inspired adversarial visual attention network (BI-AVAN) that models human visual attention from brain activity. Our model mimics the biased competition between attended and neglected objects to identify visual focus in movie frames, validated by independent eye-tracking data. Experimental results demonstrate that BI-AVAN effectively infers human visual attention and maps brain activity to visual stimuli.
<br /><br /></p>

<h4 id="core-periphery-multi-modality-feature-alignment-for-zero-shot-medical-image-analysis-ieee-transactions-on-medical-imaging-2024"><a href="https://ieeexplore.ieee.org/abstract/document/10721320/authors#authors">Core-Periphery Multi-Modality Feature Alignment for Zero-Shot Medical Image Analysis</a>, IEEE Transactions on Medical Imaging, 2024</h4>

<p><img src="static/img/research/CP.png" class="responsive" alt="Eigenmode" /></p>

<p>Simply applying language-image pre-trained CLIP to medical image analysis encounters substantial domain shifts, resulting in severe performance degradation due to inherent disparities between natural (non-medical) and medical image characteristics. To address this challenge and uphold or even enhance CLIP’s zero-shot capability in medical image analysis, we develop a novel approach, Core-Periphery feature alignment for CLIP (CP-CLIP), to model medical images and corresponding clinical text jointly. To achieve this, we design an auxiliary neural network whose structure is organized by the core-periphery (CP) principle. This auxiliary CP network not only aligns medical image and text features into a unified latent space more efficiently but also ensures alignment driven by principles of brain network organization. In this way, our approach effectively mitigates and further enhances CLIP’s zero-shot performance in medical image analysis. 
<br /><br /></p>

<h2 id="llmagi-for-healthcare"><strong>LLM/AGI for Healthcare</strong></h2>
<h4 id="deid-gpt-zero-shot-medical-text-de-identification-by-gpt-4-2023"><a href="https://arxiv.org/abs/2303.11032">DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4</a>, 2023</h4>

<p><img src="static/img/research/DeID-GPT Zero-shot Medical Text De-Identification by GPT-4.png" class="responsive" alt="Eigenmode" /></p>

<p>The advancement of large language models (LLM), such as ChatGPT and GPT-4,
have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named
entity recognition (NER) capability. In this work, we developed a novel GPT4-enabled de-identification framework
(“DeID-GPT”) to automatically identify and remove the identifying information. Compared to existing commonly
used medical text data de-identification methods, our developed DeID-GPT showed the highest accuracy and remarkable reliability in masking private information from the unstructured medical text while preserving the original
structure and meaning of the text. 
<br /><br /></p>

<h4 id="gp-gpt-large-language-model-for-gene-phenotype-mapping-2024"><a href="https://arxiv.org/abs/2409.09825">GP-GPT: Large Language Model for Gene-Phenotype Mapping</a>, 2024</h4>

<p><img src="static/img/research/GP-GPT.png" class="responsive" alt="Eigenmode" /></p>

<p>The complex traits and
heterogeneity of multi-sources genomics data poses significant challenges when adapting these
models to the bioinformatics and biomedical field.
To address these challenges, we present GPGPT, the first specialized large language model for genetic-phenotype knowledge representation
and genomics relation analysis. Our model is fine-tuned in two stages on a comprehensive corpus
composed of over 3,000,000 terms in genomics, proteomics, and medical genetics, derived from
multiple large-scale validated datasets and scientific publications. GP-GPT demonstrates proficiency in accurately retrieving medical genetics information and performing common genomics
analysis tasks, such as genomics information retrieval and relationship determination.
<br /><br /></p>

        </div>
    </div>
</div>

